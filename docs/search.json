[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n1-1 Probability Theory\n\n\n2 min\n\n\n\nMachine Learning\n\n\nStatistics\n\n\nPython\n\n\n\nProbability Theory is a branch of mathematics that studies the likelihood or chance of different outcomes in uncertain events.\n\n\n\n10/23/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1-2 Random Variable\n\n\n1 min\n\n\n\nMachine Learning\n\n\nStatistics\n\n\nPython\n\n\n\nA random variable is a mathematical concept that represents the possible numerical outcomes of a random process or experiment.\n\n\n\n10/24/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1-3 Example for Discrete Random Variable\n\n\n2 min\n\n\n\nStatistics\n\n\nPython\n\n\n\nUsing basic example to warn up your Discrete Random Variable\n\n\n\n10/25/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1-4 Example for Continuous Random Variable\n\n\n4 min\n\n\n\nStatistics\n\n\nPython\n\n\n\nUsing basic example to warn up your Continuous Random Variable\n\n\n\n10/26/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering - DBSCAN\n\n\n7 min\n\n\n\nStatistics\n\n\nPython\n\n\nMachine Learning\n\n\n\nUnderstanding how to use the DBSCAN to cluster on Mall Customers\n\n\n\n10/31/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 03 : Linear & Non-Linear Regression\n\n\n1 min\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n11/2/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 04 : Classification\n\n\n1 min\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n11/5/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 05 : Anomaly / Outlier Detection\n\n\n1 min\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n11/6/23\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "I am currently a MS Computer Science student at Virginia Tech, but prior to that, I worked as a Full-End Developer for Grant Thornton and Ernst & Young in Shanghai, Hong Kong, and Taipei. My main responsibility was to design web applications and financial operating systems, including the EY-Virtual platform. Additionally, I have expertise in utilizing Predictive Data Analytics and Machine Learning to evaluate internal controls and net assets.\nRead more about me here.\nCheck out the latest project of my github.If you enjoy the projects, please consider giving it a star✨ on Github.\nIf you want to get a notification about new content, consider subscribing here (it’s free!)"
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "I am currently a MS Computer Science student at Virginia Tech, but prior to that, I worked as a Full-End Developer for Grant Thornton and Ernst & Young in Shanghai, Hong Kong, and Taipei. My main responsibility was to design web applications and financial operating systems, including the EY-Virtual platform. Additionally, I have expertise in utilizing Predictive Data Analytics and Machine Learning to evaluate internal controls and net assets.\nRead more about me here.\nCheck out the latest project of my github.If you enjoy the projects, please consider giving it a star✨ on Github.\nIf you want to get a notification about new content, consider subscribing here (it’s free!)"
  },
  {
    "objectID": "index.html#latest-blogs",
    "href": "index.html#latest-blogs",
    "title": "Welcome To My Blog",
    "section": "Latest Blogs",
    "text": "Latest Blogs\nClick here to check out more blogs.\n\n\n\n\n  \n\n\n\n\nTask 05 : Anomaly / Outlier Detection\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2023\n\n\nEdward Xiong\n\n\n\n\n\n\n  \n\n\n\n\nTask 04 : Classification\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2023\n\n\nEdward Xiong\n\n\n\n\n\n\n  \n\n\n\n\nTask 03 : Linear & Non-Linear Regression\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\nEdward Xiong\n\n\n\n\n\n\n  \n\n\n\n\nClustering - DBSCAN\n\n\n\n\nUnderstanding how to use the DBSCAN to cluster on Mall Customers\n\n\n\n\n\n\nOct 31, 2023\n\n\nEdward Xiong\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Task_03/Task_03.html",
    "href": "posts/Task_03/Task_03.html",
    "title": "Task 03 : Linear & Non-Linear Regression",
    "section": "",
    "text": "### Task_03 Linear & Non-Linear Regression\n### Testing for Homework Environment\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndata = np.random.randn(1000)\nplt.hist(data, bins=20, color='blue', alpha=0.5)\nplt.title('Histogram of Random Data')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Probability Theory/1-3_Implementation.html",
    "href": "posts/Probability Theory/1-3_Implementation.html",
    "title": "1-3 Example for Discrete Random Variable",
    "section": "",
    "text": "Discrete Random Variable\nBased on note 1-2, we know that there are two different kinds of random variables. One is a discrete random variable, which takes on two or more distinct outcomes; the other is a continuous random variable, indicating the probability of the random variable falling within a particular interval. To help you warn up, I will provide you two basic examples and briefly explain what is the differences between them. Let’s started!!\nThe typical example is tossing the coin or rolling the dice. Here, I choose dice example to be the example. In order to get the better result, I roll the dice 100000000 times, which will close to the nature law.\n\nimport random\nimport matplotlib.pyplot as plt\n\n\n# Number of simulations for rolling the dice\nnum_simulation = 100000000;\n# Record the number of occurrences for each dice outcome\nresults = {1:0, 2:0, 3:0, 4:0, 5:0, 6:0};\n# Simulate rolling the dice\nfor _ in range(num_simulation):\n    roll_result = random.randint(1,6);\n    results[roll_result] += 1;\n# Calculate the Probability into probabilities\n\nprobabilities = {key: value / num_simulation for key, value in results.items()};\n# Demonstrate each number probabilities\nprint(\"Dice Number\\tProbabilities\");\nfor key, value in probabilities.items():\n    print(f\"{key}\\t\\t{value:.3f}\");\n\nDice Number Probabilities\n1       0.167\n2       0.167\n3       0.167\n4       0.167\n5       0.167\n6       0.167\n\n\n\n# Using Bar Chart demonstrate Probability Distribution\nplt.bar(probabilities.keys(), probabilities.values());\nplt.xlabel('Dice Number');\nplt.ylabel('Probability');\nplt.title('Dice Probability Distribution');\nplt.show();\n\n\n\n\nAs you can see, the outcome of probability is close to average (1/6)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Probability Theory/1-4_Implementation.html",
    "href": "posts/Probability Theory/1-4_Implementation.html",
    "title": "1-4 Example for Continuous Random Variable",
    "section": "",
    "text": "But how about continus random variable? The typical example in continuous random variable are height and weight. There are lots of common continous random variable in daily life, such as income, time, teamperature, speed and etc. In this section, I will use stock price to be our example because stock price can fluctuate within any real value range. I know some terminologies are hard to understand, but I will do my best to explain each of terminology.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n\n\n\n\n\n# Setting the random seed for reproducibility\nnp.random.seed(20);\n\nThe seed value, such as 20 in this example, is a non-negative integer chosen to initialize the random number generator. If a negative number is used as the seed, it may result in an error in most libraries, and it is generally recommended to use a non-negative integer as the seed value.\n\n# Setting the 100 days\ndays = 100;\n\n\\[\n\\text{Average Daily Return} = \\left( \\frac{\\text{Today's Closing Price} - \\text{Previous Day's Closing Price}}{\\text{Previous Day's Closing Price}} \\right) \\times 100\n\\]\n\n\n\n\n# Daily Return\nmean_return = 0.0005;\n\nIn financial modeling. daily return is often used as an indicator for analyzing the price movement of financial assets as it provides a more comparable measure than the price itself; Average daily represents the average of daily returns over a period of times. This assumption is often made to simplify models and capture trends in financial assets. Typically, the assumption is that the average the average daily return is constant, based on the assumption that the price movement of the asset is random, steady, and the average doesn’t change over time.\n\nvolatility = 0.02;\n\nVolatility refers to the degree of variation in the daily returns of a stock or financial asset. High volatility indicates significant daily return changes, while low volatility suggests relatively smaller changes. Volatility is an indicator of risk in the financial markets, and investors and traders often monitor changes in volatility to assess risk levels and adjust their investment portfolios. In many models, volatility is treated as a constant, but in reality, it may vary over time.\n\n# Using the Gaussian function to generate simulated daily returns\nreturns = np.random.normal(mean_return, volatility, days)\nprice = np.exp(np.cumsum(returns))\n\nHere, returns represents the daily returns, and price is the simulated stock price obtained by continuously summing up these returns.\n\n\n\nUsing the Gaussian function (or normal distribution) to simulate daily returns is based on the assumption of the Central Limit Theorem (CLT) and the simplicity and well-known statistical properties of the normal distribution.\nCentral Limit Theorem (CLT): The CLT states that the sum or average of a large number of independent and identically distributed random variables will be approximately normally distributed, regardless of the shape of the original distribution. As daily returns in financial markets are often considered the result of numerous independent and relatively small random events (e.g., trading decisions, market news), the CLT provides a theoretical foundation for using the normal distribution to model daily returns.\nSimplicity and Convenience: The Gaussian function, or normal distribution, is a simple and convenient distribution with known statistical properties. Its bell-shaped curve is easy to work with mathematically, making it a common choice for modeling financial phenomena. Using the normal distribution simplifies mathematical and statistical operations in simulation and analysis.\nThe Gaussian function, or normal distribution, is characterized by its probability density function (PDF:\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n\\]\nWhere:\n\nμ (mu) is the mean (expected value),\nσ² (sigma squared) is the variance.\n\nThe normal distribution is often referred to as the bell curve due to its characteristic shape. Its predictable properties and widespread occurrence in natural phenomena make it a common choice in financial modeling.\n\nplt.plot(price, label='Stock Price', color='b')\nplt.xlabel('Days')\nplt.ylabel('Stock Price')\nplt.title('Simulated Stock Price Movement')\nplt.legend()\nplt.show()\n\n\n\n\n\n# Plotting the probability density curve of daily returns\nplt.hist(returns, bins=30, density=True, alpha=0.6, color='b')\n\n# Using the probability density function of the normal distribution as a reference\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mean_return, volatility)\nplt.plot(x, p, 'k', linewidth=2)\n\nplt.xlabel('Daily Returns')\nplt.ylabel('Probability Density')\nplt.title('Probability Density Function of Daily Returns')\nplt.show()\n\n\n\n\nIn this example, we use the normal distribution to simulate daily returns, and then simulate the variation in stock prices by continuously accumulating these returns. It’s important to note that this is a simple simulation, and real stock prices are influenced by a more complex set of factors."
  },
  {
    "objectID": "posts/Probability Theory/1-4_Implementation.html#lets-simulate-a-stock-price-in-100-days",
    "href": "posts/Probability Theory/1-4_Implementation.html#lets-simulate-a-stock-price-in-100-days",
    "title": "1-4 Example for Continuous Random Variable",
    "section": "",
    "text": "# Setting the random seed for reproducibility\nnp.random.seed(20);\n\nThe seed value, such as 20 in this example, is a non-negative integer chosen to initialize the random number generator. If a negative number is used as the seed, it may result in an error in most libraries, and it is generally recommended to use a non-negative integer as the seed value.\n\n# Setting the 100 days\ndays = 100;\n\n\\[\n\\text{Average Daily Return} = \\left( \\frac{\\text{Today's Closing Price} - \\text{Previous Day's Closing Price}}{\\text{Previous Day's Closing Price}} \\right) \\times 100\n\\]\n\n\n\n\n# Daily Return\nmean_return = 0.0005;\n\nIn financial modeling. daily return is often used as an indicator for analyzing the price movement of financial assets as it provides a more comparable measure than the price itself; Average daily represents the average of daily returns over a period of times. This assumption is often made to simplify models and capture trends in financial assets. Typically, the assumption is that the average the average daily return is constant, based on the assumption that the price movement of the asset is random, steady, and the average doesn’t change over time.\n\nvolatility = 0.02;\n\nVolatility refers to the degree of variation in the daily returns of a stock or financial asset. High volatility indicates significant daily return changes, while low volatility suggests relatively smaller changes. Volatility is an indicator of risk in the financial markets, and investors and traders often monitor changes in volatility to assess risk levels and adjust their investment portfolios. In many models, volatility is treated as a constant, but in reality, it may vary over time.\n\n# Using the Gaussian function to generate simulated daily returns\nreturns = np.random.normal(mean_return, volatility, days)\nprice = np.exp(np.cumsum(returns))\n\nHere, returns represents the daily returns, and price is the simulated stock price obtained by continuously summing up these returns.\n\n\n\nUsing the Gaussian function (or normal distribution) to simulate daily returns is based on the assumption of the Central Limit Theorem (CLT) and the simplicity and well-known statistical properties of the normal distribution.\nCentral Limit Theorem (CLT): The CLT states that the sum or average of a large number of independent and identically distributed random variables will be approximately normally distributed, regardless of the shape of the original distribution. As daily returns in financial markets are often considered the result of numerous independent and relatively small random events (e.g., trading decisions, market news), the CLT provides a theoretical foundation for using the normal distribution to model daily returns.\nSimplicity and Convenience: The Gaussian function, or normal distribution, is a simple and convenient distribution with known statistical properties. Its bell-shaped curve is easy to work with mathematically, making it a common choice for modeling financial phenomena. Using the normal distribution simplifies mathematical and statistical operations in simulation and analysis.\nThe Gaussian function, or normal distribution, is characterized by its probability density function (PDF:\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n\\]\nWhere:\n\nμ (mu) is the mean (expected value),\nσ² (sigma squared) is the variance.\n\nThe normal distribution is often referred to as the bell curve due to its characteristic shape. Its predictable properties and widespread occurrence in natural phenomena make it a common choice in financial modeling.\n\nplt.plot(price, label='Stock Price', color='b')\nplt.xlabel('Days')\nplt.ylabel('Stock Price')\nplt.title('Simulated Stock Price Movement')\nplt.legend()\nplt.show()\n\n\n\n\n\n# Plotting the probability density curve of daily returns\nplt.hist(returns, bins=30, density=True, alpha=0.6, color='b')\n\n# Using the probability density function of the normal distribution as a reference\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mean_return, volatility)\nplt.plot(x, p, 'k', linewidth=2)\n\nplt.xlabel('Daily Returns')\nplt.ylabel('Probability Density')\nplt.title('Probability Density Function of Daily Returns')\nplt.show()\n\n\n\n\nIn this example, we use the normal distribution to simulate daily returns, and then simulate the variation in stock prices by continuously accumulating these returns. It’s important to note that this is a simple simulation, and real stock prices are influenced by a more complex set of factors."
  },
  {
    "objectID": "posts/Clustering/1-1_Clustering_DBSCAN.html",
    "href": "posts/Clustering/1-1_Clustering_DBSCAN.html",
    "title": "Clustering - DBSCAN",
    "section": "",
    "text": "Clustering is utilized on a dataset to group comparable sets of data points, discerning both shared characteristics and distinctions among the data points. This method brings together data points without relying on predefined labels, making it an unsupervised learning approach designed to reveal the inherent structure within the dataset.\nMy emphasis will be on density-based clustering techniques, particularly the DBSCAN algorithm implemented using scikit-learn. Density-based algorithms excel at identifying high-density regions and outliers, making them well-suited for tasks such as anomaly detection and clustering non-linear datasets.\nIn this note, I use the sklean library to understand the DBSCAN application.\n\n\nThe Mall Customer’s data comes from Kaggle website, which will be used mine explanation in DBSCAN. If you want to follow my notes, please download the dataset from here:\nhttps://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom IPython.display import display, HTML\n\nFirst, in order to make you analyze smoothly, I have to observe the dataset. The dataframe we called df is the key point to understand the data skema, including how many entires in the dataset, how many column in the dataset, and what’s the type of each column. Compared to using the info() method, show attribute can understand how many entries & how many column in the dataset.\nTo me, I usually use df.info() first because I can confirm whether I should do data clean or not!\n\n# Loading Data from CSV file\ndf  = pd.read_csv('../../dataset/MallCustomer/Mall_Customers.csv')\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 5 columns):\n #   Column                  Non-Null Count  Dtype \n---  ------                  --------------  ----- \n 0   CustomerID              200 non-null    int64 \n 1   Genre                   200 non-null    object\n 2   Age                     200 non-null    int64 \n 3   Annual Income (k$)      200 non-null    int64 \n 4   Spending Score (1-100)  200 non-null    int64 \ndtypes: int64(4), object(1)\nmemory usage: 7.9+ KB\n\n\nBased on the provided dataset information, my decision is to specifically focus on analyzing the correlation among Age, Annual Income, and Spending Score variables. To delve into this analysis, I have opted to allocate 80% of the dataset for trainning purposes, reserving the remaining 20% for testing. This division allows for a comprehensive exploration of the relationships between the selected variables and ensures a robust evalution of the model’s performance on unseen data during the testing phase.\n\n# Select dataframe: \nX = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\n# Training Data:\nX_train = X.iloc[:160,:]\n# Testing Data:\nX_test = X.iloc[160:,:]\n\nBefore starting to cluster the data, I want to know about the training data scatter plot. Using matplotlib to visualize the data. Here are some basci attribute or method I used. 1. scatter() –&gt; setting X scale, Y scale, label name, s mean size, c mean color; 2. title() –&gt; Graph Title 3. xlabel() –&gt; Xlabel 4. ylable() –&gt; Ylabel 5. legend() –&gt; plot intruction 6. show() –&gt; demonstrate the plot\n\n# Setting the Scale \nX_scale_Income = X_train['Annual Income (k$)'];\nX_scale_Age = X_train['Age'];\nY_scale_Score = X_train['Spending Score (1-100)'];\n\n\n# Scatter Plot - Age vs Spending Score\nplt.scatter(X_scale_Age, Y_scale_Score, label = 'Trainning Data', s = 50, c= 'blue');\nplt.title('Age vs Spending Score');\nplt.xlabel('Age');\nplt.ylabel('Spending Score(1-100)');\nplt.legend();\nplt.show();\n\n# Scatter Plot - Income vs Spending Score\nplt.scatter(X_scale_Income, Y_scale_Score, label = 'Trainning Data', s = 50, c = 'darkblue');\nplt.title('Income vs Spending Score');\nplt.xlabel('Income (k$) ');\nplt.ylabel('Spending Score(1-100)');\nplt.legend();\n\n\n\n\n\n\n\n\n\n\nAbove graph using column style to demonstrate the outcome, it’s really hard to tell what’s the differences or relations between 2 graphs in a specific area. So, I decide to using the plt.figure API & axes.ravel() to control the graphs, which not only show the graph in one row but also compare easily.\nThe biggest different in code is using axes to control the each graph.\n\n# Setting the graph & layout\n\n# Using ggplot style\n# plt.style.use('ggplot');\n# Create Graph\nfig = plt.figure('Consumption Capacity');\n# Create a layout: 2 colums in one row\naxes = fig.subplots(nrows = 1, ncols = 2);\n# Using ax1 & ax2 to control two graph separately\nax1, ax2 = axes.ravel();\n\n# Scatter Plot - Age vs Spending Score\n\nax1.scatter(X_scale_Age, Y_scale_Score, label='Training Data', s=50, c='blue')\nax1.set_title('Age vs Spending Score')\nax1.set_xlabel('Age')\nax1.set_ylabel('Spending Score (1-100)')\nax1.legend()\n\n# Scatter Plot - Income vs Spending Score\n\nax2.scatter(X_scale_Income, Y_scale_Score, label='Training Data', s=50, c='darkblue')\nax2.set_title('Income vs Spending Score')\nax2.set_xlabel('Income (k$)')\nax2.set_ylabel('Spending Score (1-100)')\nax2.legend()\n\n# Adjusting layout\n\nfig.subplots_adjust(left=0.05, bottom=0.2, right=0.95, top=0.85, hspace=0.1, wspace=0.25)\n\nplt.show()\n\n\n\n\nAfter knowing the basic graph, I just only know the basic information about Spending Score. For example, it’s quite easy to find out the age of 20 - 40 have higher score than other ages; in addition, 40K - 60K is centralizer than other cluster, which looks like their purchasing ability is normal. But, what about other cluster? How can we group efficiently? One of the clustering algorithm called DBSCAN.\n\n\n\nDBSCAN, which stands for Density-Based Spatial Clustering of Applications with Noise, is a clustering algorithm that identifies groups of data points closely positioned to each other (points with numerous nearby neighbors). Simultaneously, it labels points in regions with low density as outliers.\nDBSCAN algorithm contains to hyperparameters: - Eps : A distance threshold - min_samples : A minimum number of points\nThe DBSCAN algorithm initiates by choosing a random point from the dataset and fetching all points within Eps distance. If the count of points retrieved within the eps distance and exceeds the min_number of threshold (min_sample), the selected poins is deemed a “core point”, and a cluster is formed. The process will repeated for points until all points will be visited.\nLet’s start to try a random eps and min_samples.\n\n# pick eps & min_sample randomly\ndbscan = DBSCAN(eps = 13.5, min_samples = 5);\ndbscan.fit(X_train);\n\nWhen I adjust the parameter, I find out close to 10 - 13 in eps and min_samples around 3 - 5 can decrease the noise number. Right now, I just randomly choose the parameter.\n\nX_train = X_train.copy();\nX_train['Cluster Status'] = dbscan.labels_;\n\n\nX_train\n\n\n\n\n\n\n\n\nAge\nAnnual Income (k$)\nSpending Score (1-100)\nCluster Status\n\n\n\n\n0\n19\n15\n39\n-1\n\n\n1\n21\n15\n81\n0\n\n\n2\n20\n16\n6\n-1\n\n\n3\n23\n16\n77\n0\n\n\n4\n31\n17\n40\n0\n\n\n...\n...\n...\n...\n...\n\n\n155\n27\n78\n89\n2\n\n\n156\n37\n78\n1\n3\n\n\n157\n30\n78\n78\n2\n\n\n158\n34\n78\n1\n3\n\n\n159\n30\n78\n73\n2\n\n\n\n\n160 rows × 4 columns\n\n\n\n\n# Create subplots for visualization\nfig = plt.figure(\"DBSCAN\");\n\n(ax) = fig.subplots(nrows = 1, ncols = 2);\n\nx_income = 'Annual Income (k$)';\nx_age = 'Age';\ny_score = 'Spending Score (1-100)';\ndata = Ｘ_train[Ｘ_train['Cluster Status'] != -1];\nhue = 'Cluster Status';\n\n# Scatter plot for Age and Spending Score\nsns.scatterplot(x = x_age, \n                y = y_score,\n                data = data, \n                hue = hue,\n                legend='full', \n                palette='Set1', \n                ax=ax[0], \n                s=50)\n\n# Scatter plot for Annual Income and Spending Score\nsns.scatterplot(x = x_income, \n                y = y_score,\n                data = data, \n                hue = hue, \n                legend='full',\n                palette='Set2', \n                ax=ax[1], \n                s=50);\n\n\nfig.subplots_adjust(left=0.05, bottom=0.2, right=0.95, top=0.85, hspace=0.1, wspace=0.25)\n\n\n\n\nAs evident from the presented information, we employed the DBSCAN algorithm to perform clustering, a technique widely used in data analysis and pattern recognition. In this process, data points are grouped into clusters, and the categorization of each cluster is determined by the specified values of ‘eps’ (epsilon) and ‘min_samples.’ The parameter ‘eps’ defines the maximum distance between two data points for one to be considered as in the neighborhood of the other, while ‘min_samples’ sets the minimum number of data points required to form a dense region, which is essential for identifying meaningful clusters. This approach not only aids in understanding the inherent structure within the data but also allows for the identification of groups that may exhibit similar characteristics or behavior patterns."
  },
  {
    "objectID": "posts/Clustering/1-1_Clustering_DBSCAN.html#dataset",
    "href": "posts/Clustering/1-1_Clustering_DBSCAN.html#dataset",
    "title": "Clustering - DBSCAN",
    "section": "",
    "text": "The Mall Customer’s data comes from Kaggle website, which will be used mine explanation in DBSCAN. If you want to follow my notes, please download the dataset from here:\nhttps://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom IPython.display import display, HTML\n\nFirst, in order to make you analyze smoothly, I have to observe the dataset. The dataframe we called df is the key point to understand the data skema, including how many entires in the dataset, how many column in the dataset, and what’s the type of each column. Compared to using the info() method, show attribute can understand how many entries & how many column in the dataset.\nTo me, I usually use df.info() first because I can confirm whether I should do data clean or not!\n\n# Loading Data from CSV file\ndf  = pd.read_csv('../../dataset/MallCustomer/Mall_Customers.csv')\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 5 columns):\n #   Column                  Non-Null Count  Dtype \n---  ------                  --------------  ----- \n 0   CustomerID              200 non-null    int64 \n 1   Genre                   200 non-null    object\n 2   Age                     200 non-null    int64 \n 3   Annual Income (k$)      200 non-null    int64 \n 4   Spending Score (1-100)  200 non-null    int64 \ndtypes: int64(4), object(1)\nmemory usage: 7.9+ KB\n\n\nBased on the provided dataset information, my decision is to specifically focus on analyzing the correlation among Age, Annual Income, and Spending Score variables. To delve into this analysis, I have opted to allocate 80% of the dataset for trainning purposes, reserving the remaining 20% for testing. This division allows for a comprehensive exploration of the relationships between the selected variables and ensures a robust evalution of the model’s performance on unseen data during the testing phase.\n\n# Select dataframe: \nX = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\n# Training Data:\nX_train = X.iloc[:160,:]\n# Testing Data:\nX_test = X.iloc[160:,:]\n\nBefore starting to cluster the data, I want to know about the training data scatter plot. Using matplotlib to visualize the data. Here are some basci attribute or method I used. 1. scatter() –&gt; setting X scale, Y scale, label name, s mean size, c mean color; 2. title() –&gt; Graph Title 3. xlabel() –&gt; Xlabel 4. ylable() –&gt; Ylabel 5. legend() –&gt; plot intruction 6. show() –&gt; demonstrate the plot\n\n# Setting the Scale \nX_scale_Income = X_train['Annual Income (k$)'];\nX_scale_Age = X_train['Age'];\nY_scale_Score = X_train['Spending Score (1-100)'];\n\n\n# Scatter Plot - Age vs Spending Score\nplt.scatter(X_scale_Age, Y_scale_Score, label = 'Trainning Data', s = 50, c= 'blue');\nplt.title('Age vs Spending Score');\nplt.xlabel('Age');\nplt.ylabel('Spending Score(1-100)');\nplt.legend();\nplt.show();\n\n# Scatter Plot - Income vs Spending Score\nplt.scatter(X_scale_Income, Y_scale_Score, label = 'Trainning Data', s = 50, c = 'darkblue');\nplt.title('Income vs Spending Score');\nplt.xlabel('Income (k$) ');\nplt.ylabel('Spending Score(1-100)');\nplt.legend();"
  },
  {
    "objectID": "posts/Clustering/1-1_Clustering_DBSCAN.html#ps.-demonstrate-the-graph-in-one-row-with-two-column",
    "href": "posts/Clustering/1-1_Clustering_DBSCAN.html#ps.-demonstrate-the-graph-in-one-row-with-two-column",
    "title": "Clustering - DBSCAN",
    "section": "",
    "text": "Above graph using column style to demonstrate the outcome, it’s really hard to tell what’s the differences or relations between 2 graphs in a specific area. So, I decide to using the plt.figure API & axes.ravel() to control the graphs, which not only show the graph in one row but also compare easily.\nThe biggest different in code is using axes to control the each graph.\n\n# Setting the graph & layout\n\n# Using ggplot style\n# plt.style.use('ggplot');\n# Create Graph\nfig = plt.figure('Consumption Capacity');\n# Create a layout: 2 colums in one row\naxes = fig.subplots(nrows = 1, ncols = 2);\n# Using ax1 & ax2 to control two graph separately\nax1, ax2 = axes.ravel();\n\n# Scatter Plot - Age vs Spending Score\n\nax1.scatter(X_scale_Age, Y_scale_Score, label='Training Data', s=50, c='blue')\nax1.set_title('Age vs Spending Score')\nax1.set_xlabel('Age')\nax1.set_ylabel('Spending Score (1-100)')\nax1.legend()\n\n# Scatter Plot - Income vs Spending Score\n\nax2.scatter(X_scale_Income, Y_scale_Score, label='Training Data', s=50, c='darkblue')\nax2.set_title('Income vs Spending Score')\nax2.set_xlabel('Income (k$)')\nax2.set_ylabel('Spending Score (1-100)')\nax2.legend()\n\n# Adjusting layout\n\nfig.subplots_adjust(left=0.05, bottom=0.2, right=0.95, top=0.85, hspace=0.1, wspace=0.25)\n\nplt.show()\n\n\n\n\nAfter knowing the basic graph, I just only know the basic information about Spending Score. For example, it’s quite easy to find out the age of 20 - 40 have higher score than other ages; in addition, 40K - 60K is centralizer than other cluster, which looks like their purchasing ability is normal. But, what about other cluster? How can we group efficiently? One of the clustering algorithm called DBSCAN."
  },
  {
    "objectID": "posts/Clustering/1-1_Clustering_DBSCAN.html#dbscan",
    "href": "posts/Clustering/1-1_Clustering_DBSCAN.html#dbscan",
    "title": "Clustering - DBSCAN",
    "section": "",
    "text": "DBSCAN, which stands for Density-Based Spatial Clustering of Applications with Noise, is a clustering algorithm that identifies groups of data points closely positioned to each other (points with numerous nearby neighbors). Simultaneously, it labels points in regions with low density as outliers.\nDBSCAN algorithm contains to hyperparameters: - Eps : A distance threshold - min_samples : A minimum number of points\nThe DBSCAN algorithm initiates by choosing a random point from the dataset and fetching all points within Eps distance. If the count of points retrieved within the eps distance and exceeds the min_number of threshold (min_sample), the selected poins is deemed a “core point”, and a cluster is formed. The process will repeated for points until all points will be visited.\nLet’s start to try a random eps and min_samples.\n\n# pick eps & min_sample randomly\ndbscan = DBSCAN(eps = 13.5, min_samples = 5);\ndbscan.fit(X_train);\n\nWhen I adjust the parameter, I find out close to 10 - 13 in eps and min_samples around 3 - 5 can decrease the noise number. Right now, I just randomly choose the parameter.\n\nX_train = X_train.copy();\nX_train['Cluster Status'] = dbscan.labels_;\n\n\nX_train\n\n\n\n\n\n\n\n\nAge\nAnnual Income (k$)\nSpending Score (1-100)\nCluster Status\n\n\n\n\n0\n19\n15\n39\n-1\n\n\n1\n21\n15\n81\n0\n\n\n2\n20\n16\n6\n-1\n\n\n3\n23\n16\n77\n0\n\n\n4\n31\n17\n40\n0\n\n\n...\n...\n...\n...\n...\n\n\n155\n27\n78\n89\n2\n\n\n156\n37\n78\n1\n3\n\n\n157\n30\n78\n78\n2\n\n\n158\n34\n78\n1\n3\n\n\n159\n30\n78\n73\n2\n\n\n\n\n160 rows × 4 columns\n\n\n\n\n# Create subplots for visualization\nfig = plt.figure(\"DBSCAN\");\n\n(ax) = fig.subplots(nrows = 1, ncols = 2);\n\nx_income = 'Annual Income (k$)';\nx_age = 'Age';\ny_score = 'Spending Score (1-100)';\ndata = Ｘ_train[Ｘ_train['Cluster Status'] != -1];\nhue = 'Cluster Status';\n\n# Scatter plot for Age and Spending Score\nsns.scatterplot(x = x_age, \n                y = y_score,\n                data = data, \n                hue = hue,\n                legend='full', \n                palette='Set1', \n                ax=ax[0], \n                s=50)\n\n# Scatter plot for Annual Income and Spending Score\nsns.scatterplot(x = x_income, \n                y = y_score,\n                data = data, \n                hue = hue, \n                legend='full',\n                palette='Set2', \n                ax=ax[1], \n                s=50);\n\n\nfig.subplots_adjust(left=0.05, bottom=0.2, right=0.95, top=0.85, hspace=0.1, wspace=0.25)\n\n\n\n\nAs evident from the presented information, we employed the DBSCAN algorithm to perform clustering, a technique widely used in data analysis and pattern recognition. In this process, data points are grouped into clusters, and the categorization of each cluster is determined by the specified values of ‘eps’ (epsilon) and ‘min_samples.’ The parameter ‘eps’ defines the maximum distance between two data points for one to be considered as in the neighborhood of the other, while ‘min_samples’ sets the minimum number of data points required to form a dense region, which is essential for identifying meaningful clusters. This approach not only aids in understanding the inherent structure within the data but also allows for the identification of groups that may exhibit similar characteristics or behavior patterns."
  },
  {
    "objectID": "posts/Probability Theory/1-1_Probability Theory.html",
    "href": "posts/Probability Theory/1-1_Probability Theory.html",
    "title": "1-1 Probability Theory",
    "section": "",
    "text": "Probability Theory is a mathematical discipline that studies the likelihood of events and the rules governing their occurrence. It provides a formal framework for expressing, analyzing, and manipulating uncertainty, randomness, and chance. The fundamental concept is probability, a numerical measure ranging from 0 to 1, representing the likelihood of a particular event occurring. Probability theory is extensively used in various fields, including statistics, machine learning, physics, finance, and many others, to model and analyze uncertain situations."
  },
  {
    "objectID": "posts/Probability Theory/1-1_Probability Theory.html#probability-theory",
    "href": "posts/Probability Theory/1-1_Probability Theory.html#probability-theory",
    "title": "1-1 Probability Theory",
    "section": "",
    "text": "Probability Theory is a mathematical discipline that studies the likelihood of events and the rules governing their occurrence. It provides a formal framework for expressing, analyzing, and manipulating uncertainty, randomness, and chance. The fundamental concept is probability, a numerical measure ranging from 0 to 1, representing the likelihood of a particular event occurring. Probability theory is extensively used in various fields, including statistics, machine learning, physics, finance, and many others, to model and analyze uncertain situations."
  },
  {
    "objectID": "posts/Probability Theory/1-1_Probability Theory.html#probability-theory-and-statistics",
    "href": "posts/Probability Theory/1-1_Probability Theory.html#probability-theory-and-statistics",
    "title": "1-1 Probability Theory",
    "section": "Probability Theory and Statistics",
    "text": "Probability Theory and Statistics\n\nInferential Statistics\nProbability theory plays a crucial role in inferential statistics. One of the goals of statistics is to infer population characteristics based on sample data. Probability theory provides a framework to make inferences about population characteristics by observing the probability distribution in the sample.\n\n\nHypothesis Testing\nIn statistical analysis, hypothesis testing is a common task, involving statistical inferences about a hypothesis. Probability theory offers probability distributions and testing methods, allowing us to assess the difference between observed data and the expected outcomes under a specific hypothesis.\n\n\nConfidence Intervals\nProbability theory is also relevant to the construction of confidence intervals. Confidence intervals provide estimates for parameter values, and probability theory provides tools to quantify the uncertainty associated with such estimates."
  },
  {
    "objectID": "posts/Probability Theory/1-1_Probability Theory.html#probability-theory-and-machine-learning",
    "href": "posts/Probability Theory/1-1_Probability Theory.html#probability-theory-and-machine-learning",
    "title": "1-1 Probability Theory",
    "section": "Probability Theory and Machine Learning",
    "text": "Probability Theory and Machine Learning\n\nSupervised Learning\nIn supervised learning, models are typically trained from labeled data, and these models are then used to predict outcomes for new, unlabeled data. Probability theory concepts play a key role in constructing such models, especially in the development of probability models like Bayesian models.\n\n\nUnsupervised Learning\nIn unsupervised learning, models aim to discover patterns in data without relying on labels. Probability theory concepts such as clustering and probability density estimation find applications in unsupervised learning.\n\n\nReinforcement Learning\nReinforcement learning involves agents learning through interaction with an environment. Probability theory concepts like stochastic processes and Markov decision processes are useful for modeling such interactive processes."
  },
  {
    "objectID": "posts/Probability Theory/1-2_Random Variable.html",
    "href": "posts/Probability Theory/1-2_Random Variable.html",
    "title": "1-2 Random Variable",
    "section": "",
    "text": "A random variable is a fundamental concept in probability theory and statistics that allows us to represent and quantify uncertain outcomes of a random process or experiment. It is a variable whose value is subject to variations due to chance or randomness, providing a way to assign numerical values to the possible outcomes of a random experiment."
  },
  {
    "objectID": "posts/Probability Theory/1-2_Random Variable.html#random-variable",
    "href": "posts/Probability Theory/1-2_Random Variable.html#random-variable",
    "title": "1-2 Random Variable",
    "section": "",
    "text": "A random variable is a fundamental concept in probability theory and statistics that allows us to represent and quantify uncertain outcomes of a random process or experiment. It is a variable whose value is subject to variations due to chance or randomness, providing a way to assign numerical values to the possible outcomes of a random experiment."
  },
  {
    "objectID": "posts/Probability Theory/1-2_Random Variable.html#types-of-random-variables",
    "href": "posts/Probability Theory/1-2_Random Variable.html#types-of-random-variables",
    "title": "1-2 Random Variable",
    "section": "Types of Random Variables:",
    "text": "Types of Random Variables:\n\n1. Discrete Random Variable:\n\nA discrete random variable takes on a finite or countably infinite set of distinct values.\nThe probability distribution is described by a Probability Mass Function (PMF), specifying the probability of each possible value.\n\n\n\n2. Continuous Random Variable:\n\nA continuous random variable can take on any value within a certain range.\nThe probability distribution is described by a Probability Density Function (PDF), indicating the probability of the random variable falling within a particular interval.\n\nThe key purpose of a random variable is to enable the mathematical analysis of uncertain outcomes, allowing the application of probability theory and statistical methods. Random variables are crucial in various fields, providing a framework for modeling and understanding uncertainty in a quantitative manner."
  },
  {
    "objectID": "posts/Task_05/Task_05.html",
    "href": "posts/Task_05/Task_05.html",
    "title": "Task 05 : Anomaly / Outlier Detection",
    "section": "",
    "text": "### Task_05 Anomaly / Outlier Detection\n### Testing for Homework Environment\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndata = np.random.randn(1000)\nplt.hist(data, bins=20, color='blue', alpha=0.5)\nplt.title('Histogram of Random Data')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Task_04/Task_04.html",
    "href": "posts/Task_04/Task_04.html",
    "title": "Task 04 : Classification",
    "section": "",
    "text": "### Task_04 Classification\n### Testing for Homework Environment\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndata = np.random.randn(1000)\nplt.hist(data, bins=20, color='blue', alpha=0.5)\nplt.title('Histogram of Random Data')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "",
    "section": "",
    "text": "I am currently a MS Computer Science student at Virginia Tech, but prior to that, I worked as a Full-End Developer for Grant Thornton and Ernst & Young in Shanghai, Hong Kong, and Taipei. My main responsibility was to design web applications and financial operating systems, including the EY-Virtual platform. Additionally, I have expertise in utilizing Predictive Data Analytics and Machine Learning to evaluate internal controls and net assets.\n\n\n Back to top"
  }
]