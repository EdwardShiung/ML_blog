{
  "hash": "005d21774d0d610ea7a0ecbf32b6b902",
  "result": {
    "markdown": "---\ntitle: Clustering - DBSCAN\ntitle-block-banner: false\nauthor: Edward Xiong\ndate: '2023-10-31'\ndescription: Understanding how to use the DBSCAN to cluster on Mall Customers\nformat:\n  html:\n    toc: true\n    toc-location: right\ncategories:\n  - Statistics\n  - Python\n  - Machine Learning\nimage: ../../assets/machineLearning.jpg\ncomments:\n  giscus:\n    repo: EdwardShiung/ml_CS5805\n---\n\n# Clustering\nClustering is utilized on a dataset to group comparable sets of data points, discerning both shared characteristics and distinctions among the data points. This method brings together data points without relying on predefined labels, making it an unsupervised learning approach designed to reveal the inherent structure within the dataset.\n\nMy emphasis will be on density-based clustering techniques, particularly the DBSCAN algorithm implemented using scikit-learn. Density-based algorithms excel at identifying high-density regions and outliers, making them well-suited for tasks such as anomaly detection and clustering non-linear datasets.\n\nIn this note, I use the sklean library to understand the DBSCAN application.\n\n## Dataset\nThe Mall Customer's data comes from Kaggle website, which will be used mine explanation in DBSCAN. If you want to follow my notes, please download the dataset from here:\n\nhttps://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom IPython.display import display, HTML\n```\n:::\n\n\nFirst, in order to make you analyze smoothly, I have to observe the dataset. The dataframe we called df is the key point to understand the data skema, including how many entires in the dataset, how many column in the dataset, and what's the type of each column. Compared to using the info() method, show attribute can understand how many entries & how many column in the dataset.\n\nTo me, I usually use df.info() first because I can confirm whether I should do data clean or not!\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Loading Data from CSV file\ndf  = pd.read_csv('../../dataset/MallCustomer/Mall_Customers.csv')\ndf.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 200 entries, 0 to 199\nData columns (total 5 columns):\n #   Column                  Non-Null Count  Dtype \n---  ------                  --------------  ----- \n 0   CustomerID              200 non-null    int64 \n 1   Genre                   200 non-null    object\n 2   Age                     200 non-null    int64 \n 3   Annual Income (k$)      200 non-null    int64 \n 4   Spending Score (1-100)  200 non-null    int64 \ndtypes: int64(4), object(1)\nmemory usage: 7.9+ KB\n```\n:::\n:::\n\n\nBased on the provided dataset information, my decision is to specifically focus on analyzing the correlation among Age, Annual Income, and Spending Score variables. To delve into this analysis, I have opted to allocate 80% of the dataset for trainning purposes, reserving the remaining 20% for testing. This division allows for a comprehensive exploration of the relationships between the selected variables and ensures a robust evalution of the model's performance on unseen data during the testing phase.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Select dataframe: \nX = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\n# Training Data:\nX_train = X.iloc[:160,:]\n# Testing Data:\nX_test = X.iloc[160:,:]\n```\n:::\n\n\nBefore starting to cluster the data, I want to know about the training data scatter plot. Using matplotlib to visualize the data.\nHere are some basci attribute or method I used.\n1.  scatter() --> setting X scale, Y scale, label name, s mean size, c mean color;\n2.  title()   --> Graph Title\n3.  xlabel()  --> Xlabel\n4.  ylable()  --> Ylabel\n5.  legend()  --> plot intruction\n6.  show()    --> demonstrate the plot\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Setting the Scale \nX_scale_Income = X_train['Annual Income (k$)'];\nX_scale_Age = X_train['Age'];\nY_scale_Score = X_train['Spending Score (1-100)'];\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Scatter Plot - Age vs Spending Score\nplt.scatter(X_scale_Age, Y_scale_Score, label = 'Trainning Data', s = 50, c= 'blue');\nplt.title('Age vs Spending Score');\nplt.xlabel('Age');\nplt.ylabel('Spending Score(1-100)');\nplt.legend();\nplt.show();\n\n# Scatter Plot - Income vs Spending Score\nplt.scatter(X_scale_Income, Y_scale_Score, label = 'Trainning Data', s = 50, c = 'darkblue');\nplt.title('Income vs Spending Score');\nplt.xlabel('Income (k$) ');\nplt.ylabel('Spending Score(1-100)');\nplt.legend();\n```\n\n::: {.cell-output .cell-output-display}\n![](1-1_Clustering_DBSCAN_files/figure-html/cell-6-output-1.png){width=593 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](1-1_Clustering_DBSCAN_files/figure-html/cell-6-output-2.png){width=593 height=449}\n:::\n:::\n\n\n## ps. Demonstrate the Graph in one row with two column\n\nAbove graph using column style to demonstrate the outcome, it's really hard to tell what's the differences or relations between 2 graphs in a specific area. So, I decide to using the plt.figure API & axes.ravel() to control the graphs, which not only show the graph in one row but also compare easily.\n\nThe biggest different in code is using axes to control the each graph.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Setting the graph & layout\n\n# Using ggplot style\n# plt.style.use('ggplot');\n# Create Graph\nfig = plt.figure('Consumption Capacity');\n# Create a layout: 2 colums in one row\naxes = fig.subplots(nrows = 1, ncols = 2);\n# Using ax1 & ax2 to control two graph separately\nax1, ax2 = axes.ravel();\n\n# Scatter Plot - Age vs Spending Score\n\nax1.scatter(X_scale_Age, Y_scale_Score, label='Training Data', s=50, c='blue')\nax1.set_title('Age vs Spending Score')\nax1.set_xlabel('Age')\nax1.set_ylabel('Spending Score (1-100)')\nax1.legend()\n\n# Scatter Plot - Income vs Spending Score\n\nax2.scatter(X_scale_Income, Y_scale_Score, label='Training Data', s=50, c='darkblue')\nax2.set_title('Income vs Spending Score')\nax2.set_xlabel('Income (k$)')\nax2.set_ylabel('Spending Score (1-100)')\nax2.legend()\n\n# Adjusting layout\n\nfig.subplots_adjust(left=0.05, bottom=0.2, right=0.95, top=0.85, hspace=0.1, wspace=0.25)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](1-1_Clustering_DBSCAN_files/figure-html/cell-7-output-1.png){width=681 height=392}\n:::\n:::\n\n\nAfter knowing the basic graph, I just only know the basic information about Spending Score. For example, it's quite easy to find out the age of 20 - 40 have higher score than other ages; in addition, 40K - 60K is centralizer than other cluster, which looks like their purchasing ability is normal. But, what about other cluster? How can we group efficiently? One of the clustering algorithm called DBSCAN.\n\n## DBSCAN\nDBSCAN, which stands for Density-Based Spatial Clustering of Applications with Noise, is a clustering algorithm that identifies groups of data points closely positioned to each other (points with numerous nearby neighbors). Simultaneously, it labels points in regions with low density as outliers. \n\nDBSCAN algorithm contains to hyperparameters:\n- Eps           : A distance threshold\n- min_samples   : A minimum number of points \n\nThe DBSCAN algorithm initiates by choosing a random point from the dataset and fetching all points within Eps distance. If the count of points retrieved within the eps distance and exceeds the min_number of threshold (min_sample), the selected poins is deemed a \"core point\", and a cluster is formed. The process will repeated for points until all points will be visited.\n\nLet's start to try a random eps and min_samples.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# pick eps & min_sample randomly\ndbscan = DBSCAN(eps = 13.5, min_samples = 5);\ndbscan.fit(X_train);\n```\n:::\n\n\nWhen I adjust the parameter, I find out close to 10 - 13 in eps and min_samples around 3 - 5 can decrease the noise number. Right now, I just randomly choose the parameter.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nX_train = X_train.copy();\nX_train['Cluster Status'] = dbscan.labels_;\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nX_train\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Annual Income (k$)</th>\n      <th>Spending Score (1-100)</th>\n      <th>Cluster Status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>19</td>\n      <td>15</td>\n      <td>39</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>21</td>\n      <td>15</td>\n      <td>81</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20</td>\n      <td>16</td>\n      <td>6</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>23</td>\n      <td>16</td>\n      <td>77</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>31</td>\n      <td>17</td>\n      <td>40</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>155</th>\n      <td>27</td>\n      <td>78</td>\n      <td>89</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>156</th>\n      <td>37</td>\n      <td>78</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>157</th>\n      <td>30</td>\n      <td>78</td>\n      <td>78</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>158</th>\n      <td>34</td>\n      <td>78</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>159</th>\n      <td>30</td>\n      <td>78</td>\n      <td>73</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>160 rows × 4 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Create subplots for visualization\nfig = plt.figure(\"DBSCAN\");\n\n(ax) = fig.subplots(nrows = 1, ncols = 2);\n\nx_income = 'Annual Income (k$)';\nx_age = 'Age';\ny_score = 'Spending Score (1-100)';\ndata = Ｘ_train[Ｘ_train['Cluster Status'] != -1];\nhue = 'Cluster Status';\n\n# Scatter plot for Age and Spending Score\nsns.scatterplot(x = x_age, \n                y = y_score,\n                data = data, \n                hue = hue,\n                legend='full', \n                palette='Set1', \n                ax=ax[0], \n                s=50)\n\n# Scatter plot for Annual Income and Spending Score\nsns.scatterplot(x = x_income, \n                y = y_score,\n                data = data, \n                hue = hue, \n                legend='full',\n                palette='Set2', \n                ax=ax[1], \n                s=50);\n\n\nfig.subplots_adjust(left=0.05, bottom=0.2, right=0.95, top=0.85, hspace=0.1, wspace=0.25)\n```\n\n::: {.cell-output .cell-output-display}\n![](1-1_Clustering_DBSCAN_files/figure-html/cell-11-output-1.png){width=681 height=372}\n:::\n:::\n\n\nAs evident from the presented information, we employed the DBSCAN algorithm to perform clustering, a technique widely used in data analysis and pattern recognition. In this process, data points are grouped into clusters, and the categorization of each cluster is determined by the specified values of 'eps' (epsilon) and 'min_samples.' The parameter 'eps' defines the maximum distance between two data points for one to be considered as in the neighborhood of the other, while 'min_samples' sets the minimum number of data points required to form a dense region, which is essential for identifying meaningful clusters. This approach not only aids in understanding the inherent structure within the data but also allows for the identification of groups that may exhibit similar characteristics or behavior patterns.\n\n",
    "supporting": [
      "1-1_Clustering_DBSCAN_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}